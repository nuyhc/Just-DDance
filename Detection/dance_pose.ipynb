{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[1869]: Class CaptureDelegate is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x107d88860) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a538). One of the two will be used. Which one is undefined.\n",
      "objc[1869]: Class CVWindow is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1077b4a68) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a588). One of the two will be used. Which one is undefined.\n",
      "objc[1869]: Class CVView is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1077b4a90) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a5b0). One of the two will be used. Which one is undefined.\n",
      "objc[1869]: Class CVSlider is implemented in both /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x1077b4ab8) and /Users/nuyhc/miniforge3/envs/Deep/lib/python3.8/site-packages/cv2/cv2.abi3.so (0x15b84a5d8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pytube\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import imageio.v2 as imageio\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling Data\n",
    "틱톡 영상으로 테스트  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, res=\"720p\", path=\"./\"):\n",
    "    yt = pytube.YouTube(url)\n",
    "    stream = yt.streams.filter(res=res).first()\n",
    "    stream.download(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [블랙핑크] 요즘 유행인 지글지글 춤 in 영국 대사관 24s\n",
    "download_video(\"https://www.youtube.com/watch?v=JfGFx9tDVpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyPoint 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_gif(frames, output_name):\n",
    "    images = []\n",
    "    for frame in frames:\n",
    "        images.append(imageio.imread(frame))\n",
    "    imageio.mimsave(output_path+output_name+\".gif\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spec3\\AppData\\Local\\Temp\\ipykernel_13772\\2401414973.py:4: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread(frame))\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_style = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# TODO: OS 모듈로 영상 별로 이미지 디렉토리 만들고 저장 할 수 있게 변경\n",
    "video_name = \"220421 아이브 안유진 직캠 LOVE DIVE (IVE YUJIN FanCam)  @MCOUNTDOWN_2022421.mp4\"\n",
    "video_path = \"./\" + video_name\n",
    "\n",
    "# 비디오 로드\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# 프레임 별로 잘린 이미지\n",
    "frames_path = \"./keypoint_extraction/frames/\"\n",
    "# gif\n",
    "output_path = \"./keypoint_extraction/output/\"\n",
    "\n",
    "annotate_frames = []\n",
    "keypoint_dict = []\n",
    "\n",
    "# 바로 윈도우 열리고 출력 됨\n",
    "i = 0\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Empty Frame\")\n",
    "            break\n",
    "        # 이미지 반전 및 BGR -> RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(j): [lmk.x, lmk.y, lmk.z] for j, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_dict.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, landmark_drawing_spec=mp_drawing_style.get_default_pose_landmarks_style())\n",
    "        cv2.imwrite(frames_path+str(i)+\".png\", image)\n",
    "        annotate_frames.append(frames_path+str(i)+\".png\")\n",
    "        i += 1\n",
    "        cv2.imshow(\"Pose KeyPoint Extract: \"+video_name, image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27: break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# gif 형식으로 저장\n",
    "frames_to_gif(annotate_frames, video_name)\n",
    "\n",
    "# 키포인트 json 형식으로 저장\n",
    "with open(output_path+video_name+\"_keypoints.json\", \"w\") as fp:\n",
    "    json.dump(keypoint_dict, fp)\n",
    "    \n",
    "# 대략 2분 정도 걸리는 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# 동작의 좌표를 변환하는 메커니즘\n",
    "# 두 포즈를 비교하는 L2 규제가 효과적이라고 함\n",
    "def l2_norm(ground_relative_coords, webcam_relative_coords):\n",
    "\treturn np.linalg.norm(ground_relative_coords - webcam_relative_coords)\n",
    "\n",
    "def print_data(ground_points, webcam_points, translation_factors, w, h):\n",
    "    print(ground_points[str(11)][0:2] * np.array([w, h]) - np.array(list(translation_factors)))\n",
    "    print(webcam_points[11][0:2]* np.array([w, h]))\n",
    "\n",
    "def compare_keypoints(ground_points, webcam_points, w, h, translation_factors):\n",
    "\tground_points_array = []\n",
    "\twebcam_points_array = []\n",
    "    \n",
    "\tfor i in range(len(ground_points)):\n",
    "        # 일단 공간은 생각 안하고 x, y만 고려\n",
    "\t\tground_points_array.append(np.array(ground_points[str(i)])[0:2]* np.array([w, h]) - np.array(list(translation_factors)))\n",
    "\t\twebcam_points_array.append(np.array(webcam_points[i])[0:2]* np.array([w, h]))\n",
    "\n",
    "\tground_points_array = np.vstack(ground_points_array)\n",
    "\twebcam_points_array = np.vstack(webcam_points_array)\n",
    "\n",
    "\treturn l2_norm(ground_points_array, webcam_points_array)\n",
    "\n",
    "def connect_points(points, translation_factors, image, image_shape, scale):\n",
    "    h, w = image_shape\n",
    "    points_connect_dict = {\n",
    "        1: [2, 0],\n",
    "        2: [3],\n",
    "        3: [7],\n",
    "        4: [0, 5],\n",
    "        5: [6],\n",
    "        6: [8],\n",
    "        9: [10],\n",
    "        11: [13],\n",
    "        12: [11, 14],\n",
    "        13: [15],\n",
    "        14: [16],\n",
    "        15: [21],\n",
    "        16: [20, 14],\n",
    "        17: [15],\n",
    "        18: [20, 16],\n",
    "        19: [17],\n",
    "        20: [16],\n",
    "        22: [16],\n",
    "        23: [11, 25],\n",
    "        24: [23, 12],\n",
    "        25: [27],\n",
    "        26: [24, 28],\n",
    "        27: [31, 29],\n",
    "        28: [30, 32],\n",
    "        29: [31],\n",
    "        30: [32],\n",
    "        32: [28],\n",
    "    }\n",
    "    for p in points_connect_dict:\n",
    "        curr_point = points[str(p)][0:2]*np.array([w, h]) - np.array(list(translation_factors))\n",
    "\n",
    "        for endpoint in points_connect_dict[p]:\n",
    "            endpoint = points[str(endpoint)][0:2]*np.array([w, h]) - np.array(list(translation_factors))\n",
    "\n",
    "            cv2.line(image, (round(curr_point[0]*scale), round(curr_point[1]*scale)), (round(endpoint[0] * scale), round(endpoint[1] * scale)), (0, 0, 255), thickness=10)\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_translation_factor(gt, person, h, w):\n",
    "    x_gt, y_gt = gt['11'][0]*w, gt['11'][1]*h\n",
    "    x_person, y_person = person[11][0]*w, person[11][1]*h\n",
    "\n",
    "    if x_person >= x_gt:\n",
    "        return x_person - x_gt, y_person - y_gt\n",
    "    elif x_person <= x_gt:\n",
    "        return x_gt - x_person, y_gt - y_person\n",
    "\n",
    "\n",
    "def put_text(image, text, h, w):\n",
    "    image = cv2.putText(img=image, org=(w - 700, 50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 0, 0), text=text, thickness= 3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_style = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cv2.startWindowThread()\n",
    "# 비디오 로드\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "video_name = \"user_input_test\"\n",
    "# 프레임 별로 잘린 이미지\n",
    "frames_path = \"./keypoint_extraction/frames/\"\n",
    "# gif\n",
    "output_path = \"./keypoint_extraction/output/\"\n",
    "\n",
    "annotate_frames = []\n",
    "keypoint_dict = []\n",
    "\n",
    "# 바로 윈도우 열리고 출력 됨\n",
    "i = 0\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Empty Frame\")\n",
    "            break\n",
    "        # 이미지 반전 및 BGR -> RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(j): [lmk.x, lmk.y, lmk.z] for j, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_dict.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, landmark_drawing_spec=mp_drawing_style.get_default_pose_landmarks_style())\n",
    "        cv2.imwrite(frames_path+str(i)+\".png\", image)\n",
    "        annotate_frames.append(frames_path+str(i)+\".png\")\n",
    "        i += 1\n",
    "        cv2.imshow(\"Pose KeyPoint Extract: \"+video_name, image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27: break\n",
    "        \n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# gif 형식으로 저장\n",
    "frames_to_gif(annotate_frames, video_name)\n",
    "\n",
    "# 키포인트 json 형식으로 저장\n",
    "with open(output_path+video_name+\"_keypoints.json\", \"w\") as fp:\n",
    "    json.dump(keypoint_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 원본 영상에서 추출한 키포인트와, 입력으로 입력한 키포인트를 비교하는 방법\n",
    "2. 추출한 키포인트를 사용자 웹캠 이미지에 덮어 씌우는 방법\n",
    "3. 1, 2를 합쳐서 연습과 스코어링\n",
    "\n",
    "사용자 입력을 받는 경우와 영상에서 추출하는 경우 불필요한 부분을 제외할 방법을 고민해봐야 함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2687cd7e348f8b276a3d3877a91949627292e389c8a2a645f881205e3ac80541"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
