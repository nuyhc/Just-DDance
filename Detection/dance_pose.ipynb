{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pytube\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import imageio.v2 as imageio\n",
    "import json\n",
    "import time\n",
    "from ffpyplayer.player import MediaPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url, res=\"720p\", path=\"./\"):\n",
    "    yt = pytube.YouTube(url)\n",
    "    stream = yt.streams.filter(res=res).first()\n",
    "    stream.download(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [블랙핑크] 요즘 유행인 지글지글 춤 in 영국 대사관 24s\n",
    "download_video(\"https://www.youtube.com/watch?v=JfGFx9tDVpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyPoint 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_gif(frames, output_name):\n",
    "    images = []\n",
    "    for frame in frames:\n",
    "        images.append(imageio.imread(frame))\n",
    "    imageio.mimsave(output_path+output_name+\".gif\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spec3\\AppData\\Local\\Temp\\ipykernel_13772\\2401414973.py:4: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread(frame))\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_style = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# TODO: OS 모듈로 영상 별로 이미지 디렉토리 만들고 저장 할 수 있게 변경\n",
    "video_name = \"220421 아이브 안유진 직캠 LOVE DIVE (IVE YUJIN FanCam)  @MCOUNTDOWN_2022421.mp4\"\n",
    "video_path = \"./\" + video_name\n",
    "\n",
    "# 비디오 로드\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# 프레임 별로 잘린 이미지\n",
    "frames_path = \"./keypoint_extraction/frames/\"\n",
    "# gif\n",
    "output_path = \"./keypoint_extraction/output/\"\n",
    "\n",
    "annotate_frames = []\n",
    "keypoint_dict = []\n",
    "\n",
    "# 바로 윈도우 열리고 출력 됨\n",
    "i = 0\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Empty Frame\")\n",
    "            break\n",
    "        # 이미지 반전 및 BGR -> RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(j): [lmk.x, lmk.y, lmk.z] for j, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_dict.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, landmark_drawing_spec=mp_drawing_style.get_default_pose_landmarks_style())\n",
    "        cv2.imwrite(frames_path+str(i)+\".png\", image)\n",
    "        annotate_frames.append(frames_path+str(i)+\".png\")\n",
    "        i += 1\n",
    "        cv2.imshow(\"Pose KeyPoint Extract: \"+video_name, image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27: break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# gif 형식으로 저장\n",
    "frames_to_gif(annotate_frames, video_name)\n",
    "\n",
    "# 키포인트 json 형식으로 저장\n",
    "with open(output_path+video_name+\"_keypoints.json\", \"w\") as fp:\n",
    "    json.dump(keypoint_dict, fp)\n",
    "    \n",
    "# 대략 2분 정도 걸리는 듯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# 동작의 좌표를 변환하는 메커니즘\n",
    "# 두 포즈를 비교하는 L2 규제가 효과적이라고 함\n",
    "def l2_norm(ground_relative_coords, webcam_relative_coords):\n",
    "\treturn np.linalg.norm(ground_relative_coords - webcam_relative_coords)\n",
    "\n",
    "def print_data(ground_points, webcam_points, translation_factors, w, h):\n",
    "    print(ground_points[str(11)][0:2] * np.array([w, h]) - np.array(list(translation_factors)))\n",
    "    print(webcam_points[11][0:2]* np.array([w, h]))\n",
    "\n",
    "def compare_keypoints(ground_points, webcam_points, w, h, translation_factors):\n",
    "\tground_points_array = []\n",
    "\twebcam_points_array = []\n",
    "    \n",
    "\tfor i in range(len(ground_points)):\n",
    "        # 일단 공간은 생각 안하고 x, y만 고려\n",
    "\t\tground_points_array.append(np.array(ground_points[str(i)])[0:2]* np.array([w, h]) - np.array(list(translation_factors)))\n",
    "\t\twebcam_points_array.append(np.array(webcam_points[i])[0:2]* np.array([w, h]))\n",
    "\n",
    "\tground_points_array = np.vstack(ground_points_array)\n",
    "\twebcam_points_array = np.vstack(webcam_points_array)\n",
    "\n",
    "\treturn l2_norm(ground_points_array, webcam_points_array)\n",
    "\n",
    "def connect_points(points, translation_factors, image, image_shape, scale):\n",
    "    h, w = image_shape\n",
    "    points_connect_dict = {\n",
    "        1: [2, 0],\n",
    "        2: [3],\n",
    "        3: [7],\n",
    "        4: [0, 5],\n",
    "        5: [6],\n",
    "        6: [8],\n",
    "        9: [10],\n",
    "        11: [13],\n",
    "        12: [11, 14],\n",
    "        13: [15],\n",
    "        14: [16],\n",
    "        15: [21],\n",
    "        16: [20, 14],\n",
    "        17: [15],\n",
    "        18: [20, 16],\n",
    "        19: [17],\n",
    "        20: [16],\n",
    "        22: [16],\n",
    "        23: [11, 25],\n",
    "        24: [23, 12],\n",
    "        25: [27],\n",
    "        26: [24, 28],\n",
    "        27: [31, 29],\n",
    "        28: [30, 32],\n",
    "        29: [31],\n",
    "        30: [32],\n",
    "        32: [28],\n",
    "    }\n",
    "    for p in points_connect_dict:\n",
    "        curr_point = points[str(p)][0:2]*np.array([w, h]) - np.array(list(translation_factors))\n",
    "\n",
    "        for endpoint in points_connect_dict[p]:\n",
    "            endpoint = points[str(endpoint)][0:2]*np.array([w, h]) - np.array(list(translation_factors))\n",
    "\n",
    "            cv2.line(image, (round(curr_point[0]*scale), round(curr_point[1]*scale)), (round(endpoint[0] * scale), round(endpoint[1] * scale)), (0, 0, 255), thickness=10)\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_translation_factor(gt, person, h, w):\n",
    "    x_gt, y_gt = gt['11'][0]*w, gt['11'][1]*h\n",
    "    x_person, y_person = person[11][0]*w, person[11][1]*h\n",
    "\n",
    "    if x_person >= x_gt:\n",
    "        return x_person - x_gt, y_person - y_gt\n",
    "    elif x_person <= x_gt:\n",
    "        return x_gt - x_person, y_gt - y_person\n",
    "\n",
    "\n",
    "def put_text(image, text, h, w):\n",
    "    image = cv2.putText(img=image, org=(w - 700, 50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=2, color=(0, 0, 0), text=text, thickness= 3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_style = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cv2.startWindowThread()\n",
    "# 비디오 로드\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "video_name = \"user_input_test\"\n",
    "# 프레임 별로 잘린 이미지\n",
    "frames_path = \"./keypoint_extraction/frames/\"\n",
    "# gif\n",
    "output_path = \"./keypoint_extraction/output/\"\n",
    "\n",
    "annotate_frames = []\n",
    "keypoint_dict = []\n",
    "\n",
    "# 바로 윈도우 열리고 출력 됨\n",
    "i = 0\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Empty Frame\")\n",
    "            break\n",
    "        # 이미지 반전 및 BGR -> RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(j): [lmk.x, lmk.y, lmk.z] for j, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_dict.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, landmark_drawing_spec=mp_drawing_style.get_default_pose_landmarks_style())\n",
    "        cv2.imwrite(frames_path+str(i)+\".png\", image)\n",
    "        annotate_frames.append(frames_path+str(i)+\".png\")\n",
    "        i += 1\n",
    "        cv2.imshow(\"Pose KeyPoint Extract: \"+video_name, image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27: break\n",
    "        \n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# gif 형식으로 저장\n",
    "frames_to_gif(annotate_frames, video_name)\n",
    "\n",
    "# 키포인트 json 형식으로 저장\n",
    "with open(output_path+video_name+\"_keypoints.json\", \"w\") as fp:\n",
    "    json.dump(keypoint_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 원본 영상에서 추출한 키포인트와, 입력으로 입력한 키포인트를 비교하는 방법\n",
    "2. 추출한 키포인트를 사용자 웹캠 이미지에 덮어 씌우는 방법\n",
    "3. 1, 2를 합쳐서 연습과 스코어링\n",
    "\n",
    "사용자 입력을 받는 경우와 영상에서 추출하는 경우 불필요한 부분을 제외할 방법을 고민해봐야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "output_path = \"./\"\n",
    "video_name = \"user_input\"\n",
    "video_path = \"./\"+video_name\n",
    "\n",
    "# Open WebCam (Suppose Only 2 Maximum WebCam Install)\n",
    "cv2.startWindowThread()\n",
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "except:\n",
    "    cap = cv2.VideoCapture(1)\n",
    "# 720p\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "keypoint_face = []\n",
    "# keypoint_left_hand = []\n",
    "# keypoint_right_hand = []\n",
    "keypoint_pose = []\n",
    "\n",
    "# Init Holistic Model    \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Video Error\")\n",
    "            break\n",
    "        \n",
    "        # BGR -> RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Collect Keypoint coord.\n",
    "        if results.face_landmarks is not None:\n",
    "            annotated_face_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.face_landmarks.landmark)}\n",
    "            keypoint_face.append(annotated_face_landmarks)\n",
    "        # if results.left_hand_landmarks is not None:\n",
    "        #     annotated_left_hand_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.left_hand_landmarks.landmark)}\n",
    "        # if results.right_hand_landmarks is not None:\n",
    "        #     annotated_right_hand_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.right_hand_landmarks.landmark)}\n",
    "        if results.pose_landmarks is not None:\n",
    "            annotated_pose_landmarks = {str(idx): [lmk.x, lmk.y, lmk.z] for idx, lmk in enumerate(results.pose_landmarks.landmark)}\n",
    "            keypoint_pose.append(annotated_pose_landmarks)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Drawing Landmarks on Realtime\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=1, circle_radius=1),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 51, 51), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 204, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 51, 51), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 204, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(102, 102, 255), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(153, 153, 255), thickness=1, circle_radius=1))\n",
    "        \n",
    "        cv2.imshow(\"KeyPoint Extraction\", cv2.flip(image, 1))\n",
    "        \n",
    "        if cv2.waitKey(10)&0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(original_coor, compare_coor):\n",
    "    return np.linalg.norm(original_coor - compare_coor)\n",
    "\n",
    "def compare_origin_input(original_coor, compare_coor, w, h):\n",
    "    ori_arr = []\n",
    "    com_arr = []\n",
    "\n",
    "    for i in range(len(original_coor)):\n",
    "        ori_arr.append(np.array(original_coor[str(i)])[0:2] * np.array([w, h]))\n",
    "        com_arr.append(np.array(compare_coor[str(i)])[0:2] * np.array([w, h]))\n",
    "    ori_arr = np.vstack(ori_arr)\n",
    "    com_arr = np.vstack(com_arr)\n",
    "    return l2_norm(ori_arr, com_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1_path = \"../template/[주간아 직캠] IVE YUJIN - LOVE DIVE (아이브 유진 - 러브 다이브) l EP556.mp4_keypoints.json\"\n",
    "input_2_path = \"../template/220421 아이브 안유진 직캠 LOVE DIVE (IVE YUJIN FanCam)  @MCOUNTDOWN_2022421.mp4_keypoints.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_json(input_1_path)\n",
    "data2 = pd.read_json(input_2_path)\n",
    "\n",
    "pose_landmark_dict = {\n",
    "    # Face ###################################\n",
    "    0: \"nose\",\n",
    "    # eyes\n",
    "    1: \"left_eye_inner\",\n",
    "    2: \"left_eye\",\n",
    "    3: \"left_eye_outer\",\n",
    "    4: \"right_eye_inner\",\n",
    "    5: \"right_eye\",\n",
    "    6: \"right_eye_outer\",\n",
    "    # ears\n",
    "    7: \"left_ear\",\n",
    "    8: \"right_ear\",\n",
    "    # mouth\n",
    "    9: \"left_mouth\",\n",
    "    10: \"right_mouth\",\n",
    "    # Body ###################################\n",
    "    # shoulder\n",
    "    11: \"left_shoulder\",\n",
    "    12: \"right_shoulder\",\n",
    "    # elbow\n",
    "    13: \"left_elbow\",\n",
    "    14: \"right_elbow\",\n",
    "    # wrist\n",
    "    15: \"left_wrist\",\n",
    "    16: \"right_wrist\",\n",
    "    # hand\n",
    "    17: \"left_pinky\",\n",
    "    18: \"right_pinky\",\n",
    "    19: \"left_index\",\n",
    "    20: \"right_index\",\n",
    "    21: \"left_thumb\",\n",
    "    22: \"right_thumb\",\n",
    "    # hip\n",
    "    23: \"left_hip\",\n",
    "    24: \"right_hip\",\n",
    "    # knee\n",
    "    25: \"left_knee\",\n",
    "    26: \"right_knee\",\n",
    "    # ankle\n",
    "    27: \"left_ankle\",\n",
    "    28: \"right_ankle\",\n",
    "    # heel\n",
    "    29: \"left_heel\",\n",
    "    30: \"right_heel\",\n",
    "    # foot\n",
    "    31: \"left_foot_index\",\n",
    "    32: \"right_foot_index\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns = pose_landmark_dict.values()\n",
    "data2.columns = pose_landmark_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.470056474208831, 0.26909977197647, -0.031963795423507003]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.loc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28000330924987704, 0.13502180576324402, -0.007499510888010001]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.loc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = min(data1.shape, data2.shape)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28000330924987704, 0.13502180576324402, -0.007499510888010001]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.to_numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.97002997002997"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dance_video.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spec3\\OneDrive\\바탕 화면\\dev\\Just-DDance\\Detection\\dance_pose.ipynb 셀 23\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spec3/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/dev/Just-DDance/Detection/dance_pose.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m _, dance_image \u001b[39m=\u001b[39m dance_video\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spec3/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/dev/Just-DDance/Detection/dance_pose.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m _, user_image \u001b[39m=\u001b[39m user_video\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spec3/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/dev/Just-DDance/Detection/dance_pose.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m dance_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(dance_image, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spec3/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/dev/Just-DDance/Detection/dance_pose.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m results \u001b[39m=\u001b[39m holistic\u001b[39m.\u001b[39mprocess(dance_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spec3/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/dev/Just-DDance/Detection/dance_pose.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m dance_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(dance_image, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "url = \"./[블랙핑크] 요즘 유행인 지글지글 춤 in 영국대사관 tiktok shorts 웃참제니.mp4\"\n",
    "\n",
    "dance_video = cv2.VideoCapture(url)\n",
    "# Open WebCam (Suppose Only 2 Maximum WebCam Install)\n",
    "try:\n",
    "    user_video = cv2.VideoCapture(0)\n",
    "except:\n",
    "    user_video = cv2.VideoCapture(1)\n",
    "# 720p\n",
    "# dance_video.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "# dance_video.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "user_video.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "user_video.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# For Mac & Linux err\n",
    "cv2.startWindowThread()\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while user_video.isOpened():\n",
    "        _, dance_image = dance_video.read()\n",
    "        _, user_image = user_video.read()\n",
    "        \n",
    "        dance_image = cv2.cvtColor(dance_image, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(dance_image)\n",
    "        dance_image = cv2.cvtColor(dance_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # mp_drawing.draw_landmarks(user_image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "        #                           landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 0), thickness=1, circle_radius=1),\n",
    "        #                           connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(user_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 51, 51), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 204, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(user_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 51, 51), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 204, 204), thickness=1, circle_radius=1))\n",
    "        mp_drawing.draw_landmarks(user_image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(102, 102, 255), thickness=2, circle_radius=2),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(153, 153, 255), thickness=1, circle_radius=1))\n",
    "        \n",
    " \n",
    "        h_output = np.hstack((cv2.flip(dance_image, 1), cv2.flip(user_image, 1)))\n",
    "        cv2.imshow(\"Just DDance!\", h_output)\n",
    "        if cv2.waitKey(1)==ord(\"q\"):\n",
    "            break\n",
    "\n",
    "dance_video.release()\n",
    "user_video.release()\n",
    "# For Mac & Linux err\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dance_video.release()\n",
    "user_video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tfpy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25cc1cd7561f80357b7fa03267e13bd0c2330c203ad8a304d4fde6c3e39963dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
